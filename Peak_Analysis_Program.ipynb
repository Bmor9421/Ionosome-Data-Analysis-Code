{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disclaimer: occasionally, this program may produce some garbage data for a very small amount of peaks. However, this is always\n",
    "#easy to pick out, as it will have integration values that are far too big or far too small, or the peak integration will have\n",
    "#a different sign than the peak height, which should not be the case. This program automatically weeds out the vast majority\n",
    "#of these occurrences, but in the uncommon event it happens to miss something, it is always easy to find and manually remove.\n",
    "\n",
    "#If you do not already have access to them, the following two libraries will be required for this program to work.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumericalIntegrateData(data):\n",
    "    #Uses Trapezoid method to integrate data.\n",
    "    dt = data.Time[1] - data.Time[0]\n",
    "    \n",
    "    n = len(data)\n",
    "    \n",
    "    sum = data.Current[0]/2 + data.Current[n-1]/2\n",
    "    \n",
    "    for n in range(1, n-1):\n",
    "        sum += data.Current[n]\n",
    "        \n",
    "    sum = dt * sum\n",
    "    \n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dd = pd.read_csv(\"C:\\\\Users\\\\Bmor9421\\\\Documents\\\\Data Analysis\\\\Supporting Data\\\\Data_To_Be_Analyzed.txt\")\n",
    "#Line above has to be edited for any new data file being analyzed by changing the argument of the above function to the\n",
    "#relevant file path.\n",
    "\n",
    "#Text files read in must be formatted correctly: column names must be Current and Time, and these two names should be\n",
    "#separated by a comma and no spaces. There should be no information in the text file preceding the column names.\n",
    "\n",
    "#If you have problems with putting the file path in above and the program does not recognize the path, it may help to recall\n",
    "#that depending on usage, Python can see '\\' as an escape character, and therefore wherever your file path includes this\n",
    "#character, it may be necessary to add a second one immediately before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This next section of code is the main code which acts as the peak finding algorithm. May not be the most concise method, but it\n",
    "#works well.\n",
    "\n",
    "NOISE_LEVEL_ROUNDED_UP = 4.9e-12 #This particular value is a measure of the width of the noise at the noisiest part of the data.\n",
    "#This value is then rounded up slightly so as to \"encapsulate\" all of this noise. This is to be used as a filter for what\n",
    "#constitutes a peak, and what is simply noise. This will have to be changed for every new data set being analyzed. Finding this\n",
    "#value for each data set is best facilitated using advanced graphing software such as Igor Pro.\n",
    "\n",
    "q = 0\n",
    "peakHeight = list(np.zeros(len(dd.Current)))\n",
    "baselineAtPeak = list(np.zeros(len(dd.Current)))\n",
    "peakTime = list(np.zeros(len(dd.Current)))\n",
    "j=122142 #This particular value must always be the value at which the \"reverse signal\" in the data begins. This will have to be\n",
    "#changed for every new data set being analyzed.\n",
    "while j<len(dd.Current):\n",
    "    k=1\n",
    "    tempSum = 0 \n",
    "    if j>25 and j<len(dd.Current)-26:\n",
    "        if (abs(dd.Current[j]) > abs(dd.Current[j-1]) and abs(dd.Current[j]) > abs(dd.Current[j+1]) and abs(dd.Current[j]) > abs(dd.Current[j-2]) and abs(dd.Current[j]) > abs(dd.Current[j+2]) and abs(dd.Current[j]) > abs(dd.Current[j-3]) and abs(dd.Current[j]) > abs(dd.Current[j+3]) and abs(dd.Current[j]) > abs(dd.Current[j-4]) and abs(dd.Current[j]) > abs(dd.Current[j+4]) and abs(dd.Current[j]) > abs(dd.Current[j-5]) and abs(dd.Current[j]) > abs(dd.Current[j+5])) or (abs(dd.Current[j]) < abs(dd.Current[j-1]) and abs(dd.Current[j]) < abs(dd.Current[j+1]) and abs(dd.Current[j]) < abs(dd.Current[j-2]) and abs(dd.Current[j]) < abs(dd.Current[j+2]) and abs(dd.Current[j]) < abs(dd.Current[j-3]) and abs(dd.Current[j]) < abs(dd.Current[j+3]) and abs(dd.Current[j]) < abs(dd.Current[j-4]) and abs(dd.Current[j]) < abs(dd.Current[j+4]) and abs(dd.Current[j]) < abs(dd.Current[j-5]) and abs(dd.Current[j]) < abs(dd.Current[j+5])):\n",
    "            while k<=15:\n",
    "                tempSum = tempSum + dd.Current[(j+10)+k] + dd.Current[(j-10)-k]\n",
    "                k = k + 1\n",
    "            baselineAtPeak[q] = tempSum / 30\n",
    "            peakHeight[q] = dd.Current[j] - baselineAtPeak[q]\n",
    "            peakTime[q] = float(j)/1000\n",
    "            q = q + 1\n",
    "    j = j + 1\n",
    "\n",
    "        \n",
    "pd.set_option('display.max_rows', None)\n",
    "da=pd.DataFrame({'PeakTime': peakTime, 'PeakHeight': peakHeight, 'BaselineAtPeak': baselineAtPeak})\n",
    "\n",
    "index = da[abs(da['PeakHeight']) < NOISE_LEVEL_ROUNDED_UP].index\n",
    "da.drop(index, inplace=True)\n",
    "da.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This next section of code determines peak durations. As well, it includes several failsafes which prevent the code from \n",
    "#reaching a possible error depending on the nature of the peak. If a peak would be of the sort that would cause such an error\n",
    "#that it causes the program to reach an error, this peak is removed from the data. This does not happen to many peaks overall,\n",
    "#so the statistics obtained from this program still remain accurate in the absence of these peaks.\n",
    "\n",
    "v = 0\n",
    "leftLimit = list(np.zeros(len(da.PeakHeight)))\n",
    "rightLimit = list(np.zeros(len(da.PeakHeight)))\n",
    "peakDuration = list(np.zeros(len(da.PeakHeight)))\n",
    "\n",
    "TEMP_LEN = len(da.BaselineAtPeak)\n",
    "while v < TEMP_LEN:\n",
    "    \n",
    "    tempBaseLine = da.BaselineAtPeak[v]\n",
    "    \n",
    "    L = int(da.PeakTime[v]*1000)\n",
    "    tempL = L\n",
    "    if dd.Current[L] >= da.BaselineAtPeak[v]:\n",
    "        while dd.Current[L] >= da.BaselineAtPeak[v]:\n",
    "            L = L - 1\n",
    "            if L == 0:\n",
    "                da.drop(v, inplace=True)\n",
    "                rightLimit[v]=0\n",
    "                leftLimit[v]=0\n",
    "                break\n",
    "        if L != 0:\n",
    "            leftLimit[v] = dd.Time[L]\n",
    "            \n",
    "    L = tempL\n",
    "    if dd.Current[L] < tempBaseLine:\n",
    "        while dd.Current[L] <= da.BaselineAtPeak[v]:\n",
    "            L = L - 1\n",
    "            if L == 0:\n",
    "                da.drop(v, inplace=True)\n",
    "                rightLimit[v]=0\n",
    "                leftLimit[v]=0\n",
    "                break\n",
    "        if L != 0:\n",
    "            leftLimit[v] = dd.Time[L]    \n",
    "    \n",
    "    \n",
    "    R = int(da.PeakTime[v]*1000)\n",
    "    tempR = R\n",
    "    if dd.Current[R] >= da.BaselineAtPeak[v]:\n",
    "        while dd.Current[R] >= da.BaselineAtPeak[v]:\n",
    "            R = R + 1\n",
    "            if R==len(dd.Current):\n",
    "                da.drop(v, inplace=True)\n",
    "                rightLimit[v]=0\n",
    "                leftLimit[v]=0\n",
    "                break\n",
    "        if R!=len(dd.Current):\n",
    "            rightLimit[v] = dd.Time[R]\n",
    "            \n",
    "    R = tempR\n",
    "    if dd.Current[R] < tempBaseLine: \n",
    "        while dd.Current[R] <= da.BaselineAtPeak[v]:\n",
    "            R = R + 1\n",
    "            if R==len(dd.Current):\n",
    "                da.drop(v, inplace=True)\n",
    "                rightLimit[v]=0\n",
    "                leftLimit[v]=0\n",
    "                break\n",
    "        if R!=len(dd.Current):\n",
    "            rightLimit[v] = dd.Time[R]\n",
    "            \n",
    "    peakDuration[v] = rightLimit[v] - leftLimit[v]\n",
    "    \n",
    "    v = v + 1\n",
    "\n",
    "da.reset_index(drop = True, inplace = True)    \n",
    "    \n",
    "pd.set_option('display.max_columns', None)\n",
    "peakDuration = [i for i in peakDuration if i != 0.0]\n",
    "da.insert(3, \"PeakDuration\", peakDuration, allow_duplicates = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftLimit = [i for i in leftLimit if i != 0.0]\n",
    "rightLimit = [i for i in rightLimit if i != 0.0]\n",
    "\n",
    "da.insert(4, \"WherePeakStarts\", leftLimit, allow_duplicates = False)\n",
    "da.insert(5, \"WherePeakEnds\", rightLimit, allow_duplicates = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This next section of code calculates peak integrations for each peak. Any code that looks slightly unorthodox here is a perhaps\n",
    "#less-than-concise failsafe to keep everything running smoothly.\n",
    "\n",
    "a = 0\n",
    "Integrations = list(np.zeros(len(da.PeakHeight)))\n",
    "while a < len(da.PeakHeight):\n",
    "    q = int(da.WherePeakStarts[a]*1000)\n",
    "    i = 0\n",
    "    Current = list(np.zeros(int(da.WherePeakEnds[a]*1000 - da.WherePeakStarts[a]*1000)+2))\n",
    "    Time = list(np.zeros(int(da.WherePeakEnds[a]*1000 - da.WherePeakStarts[a]*1000)+2))\n",
    "    while q >= int(da.WherePeakStarts[a]*1000) and q <= da.WherePeakEnds[a]*1000:\n",
    "        Current[i] = dd.Current[q] - da.BaselineAtPeak[a]\n",
    "        Time[i] = q/1000\n",
    "        q = q + 1\n",
    "        i = i + 1\n",
    "    \n",
    "    data = pd.DataFrame({'Time': Time, 'Current': Current})\n",
    "    if len(data.Time) > 1: \n",
    "        Integrations[a] = NumericalIntegrateData(data)\n",
    "    a = a + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.insert(6, 'PeakIntegration', Integrations, allow_duplicates = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This next section of code drops any peaks that are clearly garbage data, which should not be many. Peaks that were far too \n",
    "#small were dropped based on the value for the noise level earlier, and peaks that are far too large or peaks whose integrations\n",
    "#are of the wrong sign are dropped here.\n",
    "\n",
    "print(\"Dropping blatantly wrong peaks based on sign discrepancies...\")\n",
    "i=0\n",
    "while i<len(da.PeakHeight):\n",
    "    if da.PeakHeight[i]/da.PeakIntegration[i] < 0:\n",
    "        da.drop(i, inplace=True)\n",
    "        print(i)\n",
    "    i = i + 1\n",
    "da.reset_index(drop = True, inplace = True)\n",
    "da\n",
    "    \n",
    "print()\n",
    "print(\"Dropping blatantly wrong peaks based on integrations being unreasonably huge...\")\n",
    "i=0\n",
    "while i<len(da.PeakIntegration):\n",
    "    if abs(da.PeakIntegration[i]) > 5e-13:\n",
    "        da.drop(i, inplace=True)\n",
    "    i = i + 1\n",
    "da.reset_index(drop = True, inplace = True)\n",
    "da\n",
    "\n",
    "print()\n",
    "#Simply calculates number of positive peaks and number of negative peaks. Might be useful data for some who use this program.\n",
    "PosPeaks = 0\n",
    "NegPeaks = 0\n",
    "for i in da.PeakHeight:\n",
    "    if i > 0:\n",
    "        PosPeaks = PosPeaks + 1\n",
    "    elif i < 0:\n",
    "        NegPeaks = NegPeaks + 1\n",
    "print(\"Number of positive peaks:\", PosPeaks)\n",
    "print(\"Number of negative peaks:\", NegPeaks)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep file location same every time, but change the sheet name to the name of the file that was just analyzed. This keeps\n",
    "#all data analysis in one Excel file, and it keeps it obvious which sheet corresponds to which data set. Note that for the\n",
    "#following line of code to work, the Excel sheet to which you are outputting your data must already exist. Simply create a blank\n",
    "#Excel sheet, name it, and then you will be able to properly use this code.\n",
    "\n",
    "with pd.ExcelWriter(r'C:\\Users\\Bmor9421\\Documents\\Data Analysis\\Peak_Data.xlsx', engine='openpyxl', mode='a') as writer: \n",
    "     da.to_excel(writer, sheet_name = 'Data_To_Be_Analyzed', index = False)\n",
    "\n",
    "#Note: This does not overwrite pre-existing sheets. So be careful about running this program for the same data set twice: there \n",
    "#will as a result be duplicate sheets in Excel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
